{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsrVCERiC-ev"
      },
      "source": [
        "Copyright (c) MONAI Consortium  \n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
        "you may not use this file except in compliance with the License.  \n",
        "You may obtain a copy of the License at  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
        "Unless required by applicable law or agreed to in writing, software  \n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
        "See the License for the specific language governing permissions and  \n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1UStEAvC-ez"
      },
      "source": [
        "# Variational autoencoder network with MedNIST Dataset\n",
        "\n",
        "This notebook illustrates the use of the variational autoencoder in MONAI for the purpose of image deblurring/denoising.\n",
        "\n",
        "# Learning objectives\n",
        "This will go through the steps of using MONAI's in-built VarAutoEncoder.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/modules/varautoencoder_mednist.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ58LeZaC-ez"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEEJOuHaC-ez",
        "outputId": "96590797-fea3-44bb-c0c1-693774a29b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "ModuleNotFoundError: No module named 'monai'\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m211.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python -c \"import monai\" || pip install -q \"monai-weekly[pillow]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv1s-hYBC-e0"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "vIo9zaoxC-e0"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pylab as pl\n",
        "from IPython import display\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import tempfile\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from urllib.request import urlretrieve\n",
        "import gzip\n",
        "from PIL import Image\n",
        "\n",
        "from monai.apps import download_and_extract\n",
        "from monai.config import print_config\n",
        "from monai.data import CacheDataset, DataLoader\n",
        "from monai.networks.nets import VarAutoEncoder\n",
        "from monai.transforms import (\n",
        "    EnsureChannelFirstD,\n",
        "    Compose,\n",
        "    LoadImageD,\n",
        "    ScaleIntensityD,\n",
        "    EnsureTypeD,\n",
        ")\n",
        "from monai.utils import set_determinism\n",
        "\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKXcZxwVC-e1"
      },
      "source": [
        "## Setup data directory\n",
        "\n",
        "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
        "This allows you to save results and reuse downloads.  \n",
        "If not specified a temporary directory will be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WFPZg7nC-e1"
      },
      "outputs": [],
      "source": [
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "if directory is not None:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
        "print(root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmVSVEsMC-e1"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "set_determinism(0)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUJoUjNGC-e2"
      },
      "outputs": [],
      "source": [
        "# Create small visualisation function\n",
        "def plot_ims(ims, shape=None, figsize=(10, 10), titles=None):\n",
        "    shape = (1, len(ims)) if shape is None else shape\n",
        "    plt.subplots(*shape, figsize=figsize)\n",
        "    for i, im in enumerate(ims):\n",
        "        plt.subplot(*shape, i + 1)\n",
        "        im = plt.imread(im) if isinstance(im, str) else torch.squeeze(im)\n",
        "        plt.imshow(im, cmap=\"gray\")\n",
        "        if titles is not None:\n",
        "            plt.title(titles[i])\n",
        "        plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJjdwCBYC-e2"
      },
      "source": [
        "# 1. Get the data\n",
        "\n",
        "This notebook can use the MedNIST or the MNIST datasets.\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "The MedNIST dataset was gathered from several sets from [TCIA](https://wiki.cancerimagingarchive.net/display/Public/Data+Usage+Policies+and+Restrictions),\n",
        "[the RSNA Bone Age Challenge](http://rsnachallenges.cloudapp.net/competitions/4),\n",
        "and [the NIH Chest X-ray dataset](https://cloud.google.com/healthcare/docs/resources/public-datasets/nih-chest).\n",
        "\n",
        "The dataset is kindly made available by [Dr. Bradley J. Erickson M.D., Ph.D.](https://www.mayo.edu/research/labs/radiology-informatics/overview) (Department of Radiology, Mayo Clinic)\n",
        "under the Creative Commons [CC BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "-m78xSywC-e2"
      },
      "outputs": [],
      "source": [
        "def get_mednist_data(mednist_folder):\n",
        "    resource = \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/MedNIST.tar.gz\"\n",
        "    md5 = \"0bc7306e7427e00ad1c5526a6677552d\"\n",
        "\n",
        "    compressed_file = os.path.join(root_dir, \"MedNIST.tar.gz\")\n",
        "    if not os.path.exists(mednist_folder):\n",
        "        download_and_extract(resource, compressed_file, root_dir, md5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFsaVR-NC-e2"
      },
      "outputs": [],
      "source": [
        "def get_mnist_data(mnist_folder):\n",
        "    if not os.path.exists(mnist_folder):\n",
        "        os.makedirs(mnist_folder)\n",
        "    if len(os.listdir(mnist_folder)) < 60000:\n",
        "        mnist_fname = os.path.join(root_dir, \"MNIST.gz\")\n",
        "        if not os.path.exists(mnist_fname):\n",
        "            mnist_train_url = \"http://yann.lecun.com/\" + \"exdb/mnist/train-images-idx3-ubyte.gz\"\n",
        "            urlretrieve(mnist_train_url, mnist_fname)\n",
        "        f = gzip.open(mnist_fname, \"r\")\n",
        "        f.read(16)  # Skip first 16 bytes\n",
        "        image_size = 28\n",
        "        num_images = 60000\n",
        "        buf = f.read(image_size * image_size * num_images)\n",
        "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
        "        data = data.reshape(num_images, image_size, image_size)\n",
        "\n",
        "    for i in trange(data.shape[0]):\n",
        "        im = np.squeeze(data[i])\n",
        "        rescaled = (255.0 / im.max() * (im - im.min())).astype(np.uint8)\n",
        "        pil_im = Image.fromarray(rescaled)\n",
        "        pil_im.save(os.path.join(mnist_folder, f\"mnist_{i}.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u-19Uh-C-e2"
      },
      "outputs": [],
      "source": [
        "use_mnist = False\n",
        "if use_mnist:\n",
        "    im_dir = os.path.join(root_dir, \"MNIST\")\n",
        "    get_mnist_data(im_dir)\n",
        "else:\n",
        "    data_dir = os.path.join(root_dir, \"MedNIST\")\n",
        "    get_mednist_data(data_dir)\n",
        "    # could be any combination of\n",
        "    # [\"AbdomenCT\", \"BreastMRI\", \"CXR\", \"ChestCT\", \"Hand\", \"HeadCT\"]\n",
        "    scan_types = [\"Hand\", \"HeadCT\"]\n",
        "    all_filenames = []\n",
        "    for scan_type in scan_types:\n",
        "        im_dir = os.path.join(data_dir, scan_type)\n",
        "        all_filenames += [os.path.join(im_dir, filename) for filename in os.listdir(im_dir)]\n",
        "random.shuffle(all_filenames)\n",
        "\n",
        "# Visualise a few of them\n",
        "rand_images = np.random.choice(all_filenames, 8, replace=False)\n",
        "plot_ims(rand_images, shape=(2, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt_qzOy_C-e3"
      },
      "outputs": [],
      "source": [
        "# Split into training and testing\n",
        "test_frac = 0.2\n",
        "num_ims = len(all_filenames)\n",
        "num_test = int(num_ims * test_frac)\n",
        "num_train = num_ims - num_test\n",
        "train_datadict = [{\"im\": fname} for fname in all_filenames[:num_train]]\n",
        "test_datadict = [{\"im\": fname} for fname in all_filenames[-num_test:]]\n",
        "print(f\"total number of images: {num_ims}\")\n",
        "print(f\"number of images for training: {len(train_datadict)}\")\n",
        "print(f\"number of images for testing: {len(test_datadict)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBhuEy_yC-e3"
      },
      "source": [
        "# 2. Create dataset and dataloader\n",
        "\n",
        "Hold data and present batches during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "JGqUI1ddC-e3"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "num_workers = 10\n",
        "\n",
        "transforms = Compose(\n",
        "    [\n",
        "        LoadImageD(keys=[\"im\"]),\n",
        "        EnsureChannelFirstD(keys=[\"im\"]),\n",
        "        ScaleIntensityD(keys=[\"im\"]),\n",
        "        EnsureTypeD(keys=[\"im\"]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_ds = CacheDataset(train_datadict, transforms, num_workers=num_workers)\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "test_ds = CacheDataset(test_datadict, transforms, num_workers=num_workers)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhlyTXTIC-e3"
      },
      "source": [
        "# 3. Train\n",
        "\n",
        "For our loss we'll want to use a combination of a reconstruction loss (here, BCE) and KLD. By increasing the importance of the KLD loss with `beta`, we encourage the network to disentangle the latent generative factors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "pmrU2risC-e3"
      },
      "outputs": [],
      "source": [
        "BCELoss = torch.nn.BCELoss(reduction=\"sum\")\n",
        "\n",
        "\n",
        "def loss_function(recon_x, x, mu, log_var, beta):\n",
        "    bce = BCELoss(recon_x, x)\n",
        "    kld = -0.5 * beta * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    return bce + kld\n",
        "\n",
        "\n",
        "def train(in_shape, max_epochs, latent_size, learning_rate, beta):\n",
        "    model = VarAutoEncoder(\n",
        "        spatial_dims=2,\n",
        "        in_shape=in_shape,\n",
        "        out_channels=1,\n",
        "        latent_size=latent_size,\n",
        "        channels=(16, 32, 64),\n",
        "        strides=(1, 2, 2),\n",
        "    ).to(device)\n",
        "\n",
        "    # Create optimiser\n",
        "    optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
        "\n",
        "    avg_train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    t = trange(max_epochs, leave=True, desc=\"epoch 0, average train loss: ?, test loss: ?\")\n",
        "    for epoch in t:\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for batch_data in train_loader:\n",
        "            inputs = batch_data[\"im\"].to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            recon_batch, mu, log_var, _ = model(inputs)\n",
        "            loss = loss_function(recon_batch, inputs, mu, log_var, beta)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        avg_train_losses.append(epoch_loss / len(train_loader.dataset))\n",
        "\n",
        "        # Test\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_data in test_loader:\n",
        "                inputs = batch_data[\"im\"].to(device)\n",
        "                recon, mu, log_var, _ = model(inputs)\n",
        "                # sum up batch loss\n",
        "                test_loss += loss_function(recon, inputs, mu, log_var, beta).item()\n",
        "        test_losses.append(test_loss / len(test_loader.dataset))\n",
        "\n",
        "        t.set_description(  # noqa: B038\n",
        "            f\"epoch {epoch + 1}, average train loss: \" f\"{avg_train_losses[-1]:.4f}, test loss: {test_losses[-1]:.4f}\"\n",
        "        )\n",
        "    return model, avg_train_losses, test_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ewc-4MR1C-e3"
      },
      "outputs": [],
      "source": [
        "max_epochs = 50\n",
        "learning_rate = 1e-4\n",
        "beta = 100  # KL beta weighting. increase for disentangled VAE\n",
        "latent_size = 2\n",
        "# VAE constructor needs image shape\n",
        "im_shape = transforms(train_datadict[0])[\"im\"].shape\n",
        "model, avg_train_losses, test_losses = train(im_shape, max_epochs, latent_size, learning_rate, beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hmaeZiDC-e3"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.title(\"Epoch losses\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "for y, label in zip([avg_train_losses, test_losses], [\"avg train loss\", \"test loss\"]):\n",
        "    x = list(range(1, len(y) + 1))\n",
        "    (line,) = plt.plot(x, y)\n",
        "    line.set_label(label)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI5MPbzYC-e4"
      },
      "source": [
        "# Scatter plot distribution\n",
        "Take each of the images in the training and test datasets and plot where they fit into the distribution (for latent size of 2 or 3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "4K8TGjxeC-e4"
      },
      "outputs": [],
      "source": [
        "# for j, loader in enumerate([train_loader, test_loader]):\n",
        "#     for i, batch_data in enumerate(loader):\n",
        "#         inputs = batch_data[\"im\"].to(device)\n",
        "#         o = model.reparameterize(*model.encode_forward(inputs)).detach().cpu().numpy()\n",
        "#         if i + j == 0:\n",
        "#             latent_coords = o\n",
        "#         else:\n",
        "#             np.vstack((latent_coords, o))\n",
        "\n",
        "# if latent_size < 4:\n",
        "#     fig = plt.figure()\n",
        "#     if latent_size == 2:\n",
        "#         plt.scatter(latent_coords[:, 0], latent_coords[:, 1], c=\"r\", marker=\"o\")\n",
        "#     elif latent_size == 3:\n",
        "#         ax = fig.add_subplot(111, projection=\"3d\")\n",
        "#         ax.scatter(latent_coords[:, 0], latent_coords[:, 1], latent_coords[:, 2], c=\"r\", marker=\"o\")\n",
        "#         ax.set_xlabel(\"dim 1\")\n",
        "#         ax.set_ylabel(\"dim 2\")\n",
        "#         ax.set_zlabel(\"dim 3\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "breast_dir = os.path.join(data_dir, \"BreastMRI\")\n",
        "filenames_breast = [os.path.join(breast_dir, filename) for filename in os.listdir(breast_dir)]\n",
        "random.shuffle(filenames_breast)\n",
        "\n",
        "breast_datadict = [{\"im\": fname} for fname in filenames_breast[:]]\n",
        "breast_ds = CacheDataset(breast_datadict, transforms, num_workers=num_workers)\n",
        "breast_loader = DataLoader(breast_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "# To show some breast images\n",
        "rand_images_breast = np.random.choice(filenames_breast, 8, replace=False)\n",
        "plot_ims(rand_images_breast, shape=(2, 4))\n"
      ],
      "metadata": {
        "id": "xaRCQX75EQEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hands and head latent space\n",
        "latent_coords = None\n",
        "\n",
        "for j, loader in enumerate([train_loader, test_loader]):\n",
        "    for i, batch_data in enumerate(loader):\n",
        "        inputs = batch_data[\"im\"].to(device)\n",
        "        o = model.reparameterize(*model.encode_forward(inputs)).detach().cpu().numpy()\n",
        "        if latent_coords is None:\n",
        "            latent_coords = o\n",
        "        else:\n",
        "            latent_coords = np.vstack((latent_coords, o))\n",
        "\n",
        "# Breast latent space\n",
        "breast_latent_coords = None\n",
        "\n",
        "for i, batch_data in enumerate(breast_loader):\n",
        "    inputs = batch_data[\"im\"].to(device)\n",
        "    o = model.reparameterize(*model.encode_forward(inputs)).detach().cpu().numpy()\n",
        "    if breast_latent_coords is None:\n",
        "        breast_latent_coords = o\n",
        "    else:\n",
        "        breast_latent_coords = np.vstack((breast_latent_coords, o))\n"
      ],
      "metadata": {
        "id": "Xk5FygXtEXBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlg9luN9C-e4"
      },
      "source": [
        "# Visualise images produced by sampling the latent space\n",
        "Visualising high dimensional data can always be tricky. If our latent size is 2, this is easy as we can vary `x` and `y` and see their effect on the generated image. For higher dimension data, one approach might be to vary two of the variables and fix the rest at 0, i.e., create a 2D slice through the N-dimensional data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPrPkjQxC-e4"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "num_ims = 10\n",
        "pl.figure(figsize=(20, 12))\n",
        "out = [[[] for _ in range(num_ims)] for _ in range(latent_size - 1)]\n",
        "dist = torch.distributions.normal.Normal(torch.tensor(0.0), torch.tensor(1.0))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for z in range(latent_size - 1):\n",
        "        for z in range(latent_size - 1):\n",
        "            for y, j in enumerate(torch.linspace(0.05, 0.95, num_ims)):\n",
        "                for i in torch.linspace(0.05, 0.95, num_ims):\n",
        "                    sample = torch.zeros(1, latent_size).to(device)\n",
        "                    sample[0, z] = dist.icdf(j)\n",
        "                    sample[0, z + 1] = dist.icdf(i)\n",
        "                    o = model.decode_forward(sample)\n",
        "                    o = o.detach().cpu().numpy().reshape(im_shape[1:])\n",
        "                    out[z][y].append(o)\n",
        "\n",
        "slices = np.block(out)\n",
        "\n",
        "%matplotlib inline\n",
        "pl.figure(figsize=(20, 12))\n",
        "for i in range(slices.shape[0]):\n",
        "    pl.imshow(slices[i])\n",
        "    pl.title(f\"slice through dims {i} and {i+1} (through centre of other dims)\")\n",
        "    if slices.shape[0] > 1:\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(pl.gcf())\n",
        "        time.sleep(0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise\n"
      ],
      "metadata": {
        "id": "zOqCvE5QEMHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if latent_size == 2:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    # Training data in red\n",
        "    plt.scatter(latent_coords[:, 0], latent_coords[:, 1], c=\"r\", marker=\"o\", label=\"Hand y HeadCT\")\n",
        "    # Breast data in blue\n",
        "    plt.scatter(breast_latent_coords[:, 0], breast_latent_coords[:, 1], c=\"b\", marker=\"^\", label=\"BreastMRI  (O.o.D)\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Dim 1\")\n",
        "    plt.ylabel(\"Dim 2\")\n",
        "    plt.title(\"Latent space: Training data vs Out-of-Distribution\")\n",
        "    plt.show()\n",
        "elif latent_size == 3:\n",
        "    from mpl_toolkits.mplot3d import Axes3D\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "    ax.scatter(latent_coords[:, 0], latent_coords[:, 1], latent_coords[:, 2], c=\"r\", marker=\"o\", label=\"Hand y HeadCT\")\n",
        "    ax.scatter(breast_latent_coords[:, 0], breast_latent_coords[:, 1], breast_latent_coords[:, 2], c=\"b\", marker=\"^\", label=\"BreastMRI (O.o.D)\")\n",
        "    ax.set_xlabel(\"Dim 1\")\n",
        "    ax.set_ylabel(\"Dim 2\")\n",
        "    ax.set_zlabel(\"Dim 3\")\n",
        "    ax.legend()\n",
        "    plt.title(\"Latent space: Training data vs Out-of-Distribution\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "PRL3eL8iEcD6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compute the reconstruction error of for some ood and training examples."
      ],
      "metadata": {
        "id": "ZgwZl_b2xFXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación\n",
        "Función de Error:\n",
        "La función compute_reconstruction_error itera sobre los datos, pasa cada batch por el modelo y calcula el error MSE entre la salida reconstruida y la imagen original. Se suma el error de cada batch y se promedia dividiendo entre el número total de muestras.\n",
        "\n",
        "Diferenciación entre Ejemplos de Entrenamiento y OOD:\n",
        "\n",
        "Ejemplos de entrenamiento: Son aquellos con los que se entrenó el modelo (por ejemplo, imágenes de cerebro y manos).\n",
        "Ejemplos OOD: Son los que el modelo no ha visto durante el entrenamiento (por ejemplo, imágenes de pechos).\n",
        "Al comparar ambos valores, normalmente el error de reconstrucción en los ejemplos OOD será mayor, ya que el modelo no ha aprendido una representación adecuada para estos datos.\n",
        "\n",
        "Este enfoque te permite cuantificar la capacidad del autoencoder para reconstruir los datos de entrada y detectar cuándo una imagen es \"extraña\" o fuera de la distribución esperada."
      ],
      "metadata": {
        "id": "s_GLzbPgxVOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_reconstruction_error(model, data_loader, device):\n",
        "    model.eval()  # Pone el modelo en modo evaluación\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            inputs = batch[\"im\"].to(device)\n",
        "            outputs, _, _, _ = model(inputs)\n",
        "\n",
        "            # Calculate MSE accumulated\n",
        "            loss = F.mse_loss(outputs, inputs, reduction=\"sum\")\n",
        "            total_loss += loss.item()\n",
        "            total_samples += inputs.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_samples  # error promedio por ejemplo\n",
        "    return avg_loss\n",
        "\n",
        "train_reconstruction_error = compute_reconstruction_error(model, train_loader, device)\n",
        "ood_reconstruction_error = compute_reconstruction_error(model, breast_loader, device)\n",
        "\n",
        "print(\"Reconstruction Error (Training examples):\", train_reconstruction_error)\n",
        "print(\"Reconstruction Error (OOD examples):\", ood_reconstruction_error)\n"
      ],
      "metadata": {
        "id": "mW3B0MoEwkwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perform an easy anomaly detection algorithm based on reconstruction error and/or latent space distribution to detect ood data."
      ],
      "metadata": {
        "id": "lMhKgaitzAHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_latent_vectors(model, data_loader, device):\n",
        "    model.eval()\n",
        "    latent_vectors = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            inputs = batch[\"im\"].to(device)\n",
        "            _, mu, _, _ = model(inputs)  # Extract `mu`\n",
        "            latent_vectors.append(mu.cpu().numpy())  # Convert to NumPy\n",
        "\n",
        "    latent_vectors = np.concatenate(latent_vectors, axis=0)\n",
        "    return latent_vectors\n",
        "\n",
        "# example: Calculate center and threshold respect to the ecludian's distance\n",
        "latent_train = get_latent_vectors(model, train_loader, device)\n",
        "center = np.mean(latent_train, axis=0)  # Centroid in the latent space\n",
        "distances_train = np.linalg.norm(latent_train - center, axis=1)  # Distance to centroid\n",
        "latent_threshold = np.mean(distances_train) + 2 * np.std(distances_train)  # Threshold\n",
        "print(\"threshold in latent space:\", latent_threshold)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def detect_anomalies_with_images(model, data_loader, device, center, latent_threshold):\n",
        "    model.eval()\n",
        "    anomalies = []\n",
        "    anomaly_images = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            inputs = batch[\"im\"].to(device)\n",
        "            _, mu, _, _ = model(inputs)  # extarct the latent vectors\n",
        "            latent_vectors = mu.cpu().numpy()\n",
        "\n",
        "            # Calculate the distance to centroid\n",
        "            distances = np.linalg.norm(latent_vectors - center, axis=1)\n",
        "\n",
        "            # Tag as anomaly if the distance is higher than the threshold\n",
        "            anomaly_labels = distances > latent_threshold\n",
        "\n",
        "            # Save anomalies\n",
        "            for i, is_anomaly in enumerate(anomaly_labels):\n",
        "                if is_anomaly:\n",
        "                    anomalies.append(True)\n",
        "                    anomaly_images.append(inputs[i].cpu().numpy())  # Convert to NumPy\n",
        "                else:\n",
        "                    anomalies.append(False)\n",
        "\n",
        "    return np.array(anomalies), anomaly_images\n",
        "\n",
        "# Detect and get anomalies\n",
        "anomaly_labels, anomaly_images = detect_anomalies_with_images(model, test_loader, device, center, latent_threshold)\n",
        "\n",
        "# Show anomaly images\n",
        "num_images_to_show = min(10, len(anomaly_images))\n",
        "fig, axes = plt.subplots(1, num_images_to_show, figsize=(15, 5))\n",
        "\n",
        "for i in range(num_images_to_show):\n",
        "    axes[i].imshow(anomaly_images[i][0])\n",
        "    axes[i].axis(\"off\")\n",
        "    axes[i].set_title(f\"Anomaly {i+1}\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "6edQINdIy--x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2hS09zYC-e4"
      },
      "source": [
        "## Cleanup data directory\n",
        "\n",
        "Remove directory if a temporary was used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "odJUFrdGC-e4"
      },
      "outputs": [],
      "source": [
        "if directory is None:\n",
        "    shutil.rmtree(root_dir)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}